{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_avVIGTpvioI"
      },
      "source": [
        "# DX 704 Week 3 Project\n",
        "\n",
        "This week's project will give you practice with optimizing choices for bandit algorithms.\n",
        "You will be given access to the bandit problem via a blackbox object, and you will investigate the bandit rewards to pick a suitable algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftRkegOQowWA"
      },
      "source": [
        "The full project description, a template notebook and supporting code are available on GitHub: [Project 3 Materials](https://github.com/bu-cds-dx704/dx704-project-03).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tPHvNSEdR6h"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6VKDAEY8JMI"
      },
      "source": [
        "## Part 1: Pick a Bandit Algorithm\n",
        "\n",
        "Experiment with the multi-armed bandit interface using seed 0 to learn about the distribution of rewards and decide what kind of bandit algorithm will be appropriate.\n",
        "A histogram will likely be helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BCggNE7NpiQN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /home/codespace/.python/current/lib/python3.12/site-packages (2.4.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "%pip install numpy \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class BanditProblem(object):\n",
        "    def __init__(self, seed):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.num_arms = 3\n",
        "        self.ns = self.rng.integers(low=1, high=10, size=self.num_arms)\n",
        "        self.ps = self.rng.uniform(low=0.2, high=0.4, size=self.num_arms)\n",
        "\n",
        "    def get_num_arms(self):\n",
        "        return self.num_arms\n",
        "\n",
        "    def get_reward(self, arm):\n",
        "        if arm < 0 or arm >= self.num_arms:\n",
        "            raise ValueError(\"Invalid arm\")\n",
        "\n",
        "        x = self.rng.uniform()\n",
        "        x *= self.rng.binomial(self.ns[arm], self.ps[arm])\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X99ZQUyhpgak"
      },
      "outputs": [],
      "source": [
        "bandit0 = BanditProblem(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "frDtVjt4qATJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_num_arms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdM9Ec3HqC6h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.8255111545554434"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_reward(arm=0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iuQ0jCr_plcZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "text = (\"UCB1 (Upper Confidence Bound) — rewards are bounded, continuous-valued (not just 0/1), \"\n",
        "        \"so UCB1 is appropriate for bounded stationary rewards, while Bernoulli Thompson Sampling \"\n",
        "        \"doesn’t match the reward type and epsilon-greedy is less efficient because it explores uniformly \"\n",
        "        \"rather than using confidence bounds.\\n\")\n",
        "\n",
        "with open(\"algorithm-choice.txt\", \"w\") as f:\n",
        "    f.write(text) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-3KtNXtzlY"
      },
      "source": [
        "Based on your investigation, pick an appropriate bandit algorithm to implement from the algorithms covered this week.\n",
        "Write a file \"algorithm-choice.txt\" that states your choice and give a single sentence justifying your choice and rejecting the alternatives.\n",
        "Keep your explanation concise; you should be able to justify your choice solely based on the type of numbers observed, and whether those match the bandit algorithms that you have learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY_xvfK4rN0C"
      },
      "source": [
        "## Part 2: Implement Bandit\n",
        "\n",
        "Based on your decision, implement an appropriate bandit algorithm and pick 1000 actions using seed 2026."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kufc5pAPrWTT"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class UCB1Agent:\n",
        "    def __init__(self, n_arms, reward_bound=9.0):\n",
        "        self.n_arms = n_arms\n",
        "        self.B = float(reward_bound)\n",
        "        self.counts = np.zeros(n_arms, dtype=int)\n",
        "        self.sums = np.zeros(n_arms, dtype=float)  # normalized reward sums\n",
        "\n",
        "    def select_arm(self):\n",
        "        for i in range(self.n_arms):\n",
        "            if self.counts[i] == 0:\n",
        "                return i\n",
        "\n",
        "        t = int(self.counts.sum())\n",
        "        best_i = 0\n",
        "        best_ucb = -float(\"inf\")\n",
        "\n",
        "        for i in range(self.n_arms):\n",
        "            avg = self.sums[i] / self.counts[i]\n",
        "            bonus = math.sqrt(2.0 * math.log(t) / self.counts[i])\n",
        "            ucb = avg + bonus\n",
        "            if ucb > best_ucb:\n",
        "                best_ucb = ucb\n",
        "                best_i = i\n",
        "\n",
        "        return best_i\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        r = float(reward) / self.B  # normalize to [0,1]\n",
        "        self.counts[arm] += 1\n",
        "        self.sums[arm] += r\n",
        "\n",
        "\n",
        "# --- run 1000 actions ---\n",
        "bandit = BanditProblem(seed=2026)\n",
        "K = bandit.get_num_arms()\n",
        "agent = UCB1Agent(n_arms=K, reward_bound=9.0)\n",
        "\n",
        "T = 1000\n",
        "history_actions = []\n",
        "history_rewards = []\n",
        "\n",
        "for _ in range(T):\n",
        "    a = agent.select_arm()\n",
        "    r = bandit.get_reward(a)\n",
        "    agent.update(a, r)\n",
        "    history_actions.append(a)\n",
        "    history_rewards.append(r) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho9QihatrZqy"
      },
      "source": [
        "Write a file \"history.tsv\" with columns action and reward in the order that the actions were taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OsiU7S1XrX0q"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "with open(\"history.tsv\", \"w\") as f:\n",
        "    f.write(\"action\\treward\\n\")\n",
        "    for a, r in zip(history_actions, history_rewards):\n",
        "        f.write(f\"{a}\\t{r}\\n\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwm-1x3mrfXu"
      },
      "source": [
        "Submit \"history.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0xYgCzrmGj"
      },
      "source": [
        "## Part 3: Action Statistics\n",
        "\n",
        "Based on the data from part 2, estimate the expected reward for each arm and write a file \"actions.tsv\" with the columns action, min_reward, mean_reward, max_reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a-uAbY03sFna"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Created history.tsv and actions.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# --- UCB1 Agent ---\n",
        "class UCB1Agent:\n",
        "    def __init__(self, n_arms, reward_bound=9.0):\n",
        "        self.n_arms = n_arms\n",
        "        self.B = float(reward_bound)\n",
        "        self.counts = np.zeros(n_arms, dtype=int)\n",
        "        self.sums = np.zeros(n_arms, dtype=float)  # normalized reward sums\n",
        "\n",
        "    def select_arm(self):\n",
        "        for i in range(self.n_arms):\n",
        "            if self.counts[i] == 0:\n",
        "                return i\n",
        "\n",
        "        t = int(self.counts.sum())\n",
        "        best_i = 0\n",
        "        best_ucb = -float(\"inf\")\n",
        "\n",
        "        for i in range(self.n_arms):\n",
        "            avg = self.sums[i] / self.counts[i]\n",
        "            bonus = math.sqrt(2.0 * math.log(t) / self.counts[i])\n",
        "            ucb = avg + bonus\n",
        "            if ucb > best_ucb:\n",
        "                best_ucb = ucb\n",
        "                best_i = i\n",
        "\n",
        "        return best_i\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        r = float(reward) / self.B  # normalize to [0,1]\n",
        "        self.counts[arm] += 1\n",
        "        self.sums[arm] += r\n",
        "\n",
        "\n",
        "# --- Run bandit for 1000 steps ---\n",
        "bandit = BanditProblem(seed=2026)\n",
        "K = bandit.get_num_arms()\n",
        "agent = UCB1Agent(n_arms=K, reward_bound=9.0)\n",
        "\n",
        "T = 1000\n",
        "history_actions = []\n",
        "history_rewards = []\n",
        "\n",
        "for _ in range(T):\n",
        "    a = agent.select_arm()\n",
        "    r = bandit.get_reward(a)\n",
        "    agent.update(a, r)\n",
        "    history_actions.append(a)\n",
        "    history_rewards.append(r)\n",
        "\n",
        "# --- Write history.tsv ---\n",
        "with open(\"history.tsv\", \"w\") as f:\n",
        "    f.write(\"action\\treward\\n\")\n",
        "    for a, r in zip(history_actions, history_rewards):\n",
        "        f.write(f\"{a}\\t{r}\\n\")\n",
        "\n",
        "# --- Compute per-arm stats ---\n",
        "rewards_by_arm = {a: [] for a in range(K)}\n",
        "for a, r in zip(history_actions, history_rewards):\n",
        "    rewards_by_arm[a].append(r)\n",
        "\n",
        "arm_stats = []\n",
        "for a in range(K):\n",
        "    arr = np.array(rewards_by_arm[a], dtype=float)\n",
        "    min_r = float(arr.min()) if len(arr) > 0 else 0.0\n",
        "    mean_r = float(arr.mean()) if len(arr) > 0 else 0.0\n",
        "    max_r = float(arr.max()) if len(arr) > 0 else 0.0\n",
        "    arm_stats.append((a, min_r, mean_r, max_r))\n",
        "\n",
        "# --- Write actions.tsv ---\n",
        "with open(\"actions.tsv\", \"w\") as f:\n",
        "    f.write(\"action\\tmin_reward\\tmean_reward\\tmax_reward\\n\")\n",
        "    for a, min_r, mean_r, max_r in arm_stats:\n",
        "        f.write(f\"{a}\\t{min_r}\\t{mean_r}\\t{max_r}\\n\")\n",
        "\n",
        "print(\"Done! Created history.tsv and actions.tsv\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk8s1hpEsHWX"
      },
      "source": [
        "Submit \"actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asaIrLTtsKEv"
      },
      "source": [
        "## Part 4: Regret Estimates\n",
        "\n",
        "Estimate the regret taking 1000 actions with the following strategies.\n",
        "\n",
        "* uniform: Pick an arm uniformly at random.\n",
        "* just-i: Always pick arm $i$. Do this for $i=0$ to $K-1$ where $K$ is the number of arms.\n",
        "* actual: This should match your output in part 2.\n",
        "\n",
        "These estimates should be based on your previous action statistics; you should not use the true action values from the bandit code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LgCSJKDmso5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated best arm: 0 with mean 1.240236\n",
            "\n",
            "uniform   regret=688.536220\n",
            "just-0    regret=0.000000\n",
            "just-1    regret=949.552759\n",
            "just-2    regret=1116.055901\n",
            "actual    regret=405.709918\n",
            "\n",
            "Wrote regret.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ----- Inputs assumed to exist from earlier parts -----\n",
        "# arm_stats: list of tuples (action, min_reward, mean_reward, max_reward)\n",
        "# history_actions: list of actions taken in Part 2 (length 1000)\n",
        "\n",
        "T = len(history_actions)\n",
        "\n",
        "# Estimated mean reward for each arm (from Part 3)\n",
        "means = {a: mean_r for (a, min_r, mean_r, max_r) in arm_stats}\n",
        "K = len(means)\n",
        "\n",
        "best_arm = max(means, key=lambda a: means[a])\n",
        "best_mean = means[best_arm]\n",
        "\n",
        "def regret_from_expected_reward(expected_reward_per_pull):\n",
        "    # Regret over T pulls = T*(mu* - E[mu_strategy])\n",
        "    return T * (best_mean - expected_reward_per_pull)\n",
        "\n",
        "# 1) uniform: choose each arm with prob 1/K\n",
        "uniform_expected = sum(means[a] for a in range(K)) / K\n",
        "uniform_regret = regret_from_expected_reward(uniform_expected)\n",
        "\n",
        "# 2) just-i: always choose arm i\n",
        "just_regrets = []\n",
        "for i in range(K):\n",
        "    just_expected = means[i]\n",
        "    just_regrets.append((f\"just-{i}\", regret_from_expected_reward(just_expected)))\n",
        "\n",
        "# 3) actual: use the action frequencies from Part 2 (NOT the realized rewards)\n",
        "counts = np.bincount(np.array(history_actions, dtype=int), minlength=K)\n",
        "actual_expected = sum((counts[i] / T) * means[i] for i in range(K))\n",
        "actual_regret = regret_from_expected_reward(actual_expected)\n",
        "\n",
        "# Collect results\n",
        "results = [(\"uniform\", uniform_regret)] + just_regrets + [(\"actual\", actual_regret)]\n",
        "\n",
        "# Print nicely\n",
        "print(f\"Estimated best arm: {best_arm} with mean {best_mean:.6f}\\n\")\n",
        "for name, reg in results:\n",
        "    print(f\"{name:8s}  regret={reg:.6f}\")\n",
        "\n",
        "# Write regret estimates to file (handy for Gradescope if they expect it)\n",
        "with open(\"regret.tsv\", \"w\") as f:\n",
        "    f.write(\"strategy\\tregret\\n\")\n",
        "    for name, reg in results:\n",
        "        f.write(f\"{name}\\t{reg}\\n\")\n",
        "\n",
        "print(\"\\nWrote regret.tsv\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncXs2IqPsqQO"
      },
      "source": [
        "Write your results to a file \"strategies.tsv\" with the columns strategy and regret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GlYK-oCUtyFm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote strategies.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "with open(\"strategies.tsv\", \"w\") as f:\n",
        "    f.write(\"strategy\\tregret\\n\")\n",
        "    for name, reg in results:\n",
        "        f.write(f\"{name}\\t{reg}\\n\")\n",
        "\n",
        "print(\"Wrote strategies.tsv\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNs9BJCvtz2N"
      },
      "source": [
        "Submit \"strategies.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lopxdy3lsysb"
      },
      "source": [
        "## Part 5: Acknowledgments\n",
        "\n",
        "Make a file \"acknowledgments.txt\" documenting any outside sources or help on this project.\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for.\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy.\n",
        "If no acknowledgements are appropriate, just write none in the file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote acknowledgments.txt\n"
          ]
        }
      ],
      "source": [
        "with open(\"acknowledgments.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"I referenced course materials and example notebooks to review multi-armed bandit algorithms and used ChatGPT as a reference tool \"\n",
        "        \"to sanity-check my understanding and help debug and refine my Python code. No outside collaborators were involved.\\n\"\n",
        "    )\n",
        "\n",
        "print(\"Wrote acknowledgments.txt\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-GaDpOw06W"
      },
      "source": [
        "Submit \"acknowledgments.txt\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AR_XyZi8N_Q"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXhGo_008M-b"
      },
      "source": [
        "Submit \"project.ipynb\" in Gradescope."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
